<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--suppress CheckImageSize -->
<html xmlns="http://www.w3.org/1999/xhtml">

  <head>

    <script src="js/head.js?prefix="></script>
    <meta name="description" content="TVNet">

    <title>TVNet</title>
    <link rel="stylesheet" href="css/font.css">
    <link rel="stylesheet" href="css/main.css">

  </head>

  <body>

    <div class="outercontainer">
      <div class="container">

        <div class="content project_title">
          <h1>End-to-End Learning of Motion Representation for Video Understanding</h1>
        </div>

        <div class="content project_headline">
          <div class="img" style="text-align:center">
            <img class="img_responsive" src="images/TVNet.png" alt="TVNet" style="margin:auto;max-width:80%"/>
          </div>
          <div class="text">
            <p>Figure 1: The illustration of the process for unfolding TV-L1 to TVNet. For TV-L1, we illustrate each iteration of TV-L1. We reformulate the bicubic warping, gradient and divergence computations in TV-L1 to bilinear warping and convolution operations in TVNet.</p>
          </div>
        </div>
        <!--<div class="content project_headline">-->
          <!--<div class="img" style="text-align:center">-->
            <!--<img class="img_responsive" src="images/results.jpg" alt="TVNet Result" style="margin:auto;max-width:80%"/>-->
          <!--</div>-->
          <!--<div class="text">-->
            <!--<p>Figure 2: TVNet result</p>-->
          <!--</div>-->
        <!--</div>-->

        <div class="content">

        <div class="content">
          <p class="text">
            <h3>Abstract</h3>
            <p>Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network to learn optical-flow-like features from data.
              TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers.TVNet can therefore be used directly without any extra learning.
              <p>Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk.
              Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition and one action similarity detection benchmarks verify the effectiveness of the proposed approach. Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time.</p>
              </p>
          </div>
        </div>

        <!--<div class="content">-->
          <!--<div class="text">-->
            <!--<h3>Spotlight Video</h3>-->
          <!--</div>-->
          <!--<div class="project_headline">-->
            <!--<iframe width="720" height="405" src="https://www.youtube.com" frameborder="0" allowfullscreen></iframe>-->
            <!--<p>If you cannot access YouTube, please <a href="videos/">download our video here</a>.</p>-->
          <!--</div>-->
        <!--</div>-->

        <div class="content">
          <!-- <div class="text"> -->
            <h3>Publication</h3>
            <ul>
              <li>
              <div class="title"><a name="TVNet_cvpr">End-to-End Learning of Motion Representation for Video Understanding</a></div>
                <div class="authors">
                  <a href="http://lijiefan.me">Lijie Fan</a>*,
                  <a href="https://sites.google.com/site/wenbinghuangshomepage/home">Wenbing Huang</a>*,
                  Chuang Gan,
                  <a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a>,
                  <a href="http://boqinggong.info">Boqing Gong</a>, and
                  <a href="http://ranger.uta.edu/~huang/">Junzhou Huang</a>
                </div>
                <div>
                  <span class="venue">CVPR 2018 (spotlight) </span>
                  <span class="tag"><a href="papers/TVNet_cvpr.pdf">Paper</a></span>
                  <!--<span class="tag"><a href="talks/poster_cvpr.pdf">Poster</a></span>-->
                  <span class="tag"><a href="https://arxiv.org/abs/1804.00413">arXiv</a></span>
                  <span class="tag"><a href="bibtex/TVNet_cvpr.bib">BibTeX</a></span>
                  <span class="tag"><a href="https://github.com/LijieFan/tvnet">code</a></span>
                  (* indicates equal contributions)
                </div>
              </li>
            </ul>
          <!-- </div> -->
        </div>

        <!--<div class="content">-->
          <!--<div class="text">-->
            <!--<h3>Downloads</h3>-->
			      <!--<ul class="download">-->
              <!--<li><a href="">Pretrained models and code for TVNet</a></li>-->
			      <!--</ul>-->
          <!--</div>-->
        <!--</div>-->

      </div>
    </div>

  </body>

</html>
